# Twitter Data Scraper and Analyzer

This project provides tools for scraping and analyzing Twitter data using Playwright and Pandas. It consists of two main components:

1.  **`twitter_scraping_cmds.py`:** A Python script that uses Playwright to scrape user profile information and a specified number of tweets from a given Twitter profile URL.
2.  **`twitter_scraping.ipynb`:** A Jupyter Notebook that loads the scraped tweet data (from a JSON file generated by `twitter_scraping_cmds.py`), extracts relevant information, performs analysis (identifies tweets with above-average engagement), and displays/saves the results.

## Table of Contents

*   [Features](#features)
*   [Requirements](#requirements)
*   [Usage](#usage)
    *   [1. Scraping Twitter Data (`twitter_scraping_cmds.py`)](#1-scraping-twitter-data-twitter_scraping_cmdspy)
    *   [2. Analyzing Tweet Data (`twitter_scraping.ipynb`)](#2-analyzing-tweet-data-twitter_scrapingipynb)
*   [Code Structure](#code-structure)
    *   [`twitter_scraping_cmds.py`](#twitter_scraping_cmdspy)
    *   [`twitter_scraping.ipynb`](#twitter_scrapingipynb)
*   [Notes and Considerations](#notes-and-considerations)
*   [Disclaimer](#disclaimer)

## Features

*   **Scrape User Profile Data:** Retrieves comprehensive user profile information, including:
    *   Username
    *   Display name
    *   Bio
    *   Follower/following counts
    *   Verification status
    *   Profile image URL
    *   Banner image URL
    *   Location
    *   Creation date
    *   And more...
*   **Scrape Recent Tweets:** Fetches a configurable number of recent tweets from a user's timeline.
*   **Tweet Analysis:** Analyzes scraped tweets to identify those with above-average engagement (favorites, retweets, replies).
*   **Data Extraction:** Extracts key information from the raw JSON data returned by Twitter's API, including:
    *   Tweet text
    *   Favorite count
    *   Retweet count
    *   Reply count
    *   Quote count
    *   Author name
    *   Author's follower count
*   **Data Output:**
    *   Saves scraped data to JSON files.
    *   Presents analyzed tweet data in a Pandas DataFrame.
    *   Displays results in a user-friendly format in the console.
    *   Saves analyzed data (including averages) to a CSV file.
*   **Dynamic Filenames:**  Generates output filenames based on the target Twitter username and the number of tweets requested.
*   **Configurable:** Uses a simple configuration section within the script to set the target profile URL and the number of tweets to retrieve.
*   **Robust Error Handling:** Includes comprehensive error handling to deal with network issues, page loading problems, and variations in Twitter's API responses. Uses timeouts and retries to maximize reliability.
* **Jupyter Notebook analysis:** Includes a Jupyter Notebook for easy analysis.

## Requirements

*   Python 3.7+
*   `playwright`:  `pip install playwright`
*   `scrapfly-sdk`: `pip install scrapfly-sdk`  (Not strictly required for core functionality, but used in the original setup)
*   `pandas`: `pip install pandas`
* Playwright browsers: `playwright install`

## Usage

### 1. Scraping Twitter Data (`twitter_scraping_cmds.py`)

1.  **Configuration:**
    *   Open `twitter_scraping_cmds.py` in a text editor.
    *   Modify the `PROFILE_URL` variable to the URL of the Twitter profile you want to scrape (e.g., `PROFILE_URL = "https://x.com/elonmusk"`).
    *   Modify the `NUM_POSTS_TO_RETRIEVE` variable to the desired number of tweets to fetch (e.g., `NUM_POSTS_TO_RETRIEVE = 20`).

2.  **Run the Scraper:**
    *   Open a terminal or command prompt.
    *   Navigate to the directory where you saved `twitter_scraping_cmds.py`.
    *   Run the script: `python twitter_scraping_cmds.py`

3.  **Output:**
    *   The script will create two JSON files in the same directory:
        *   `{username}_first_{N}_tweets.json`: Contains the raw JSON data for the scraped tweets (where `{username}` is the Twitter username and `{N}` is the number of tweets).
        *   `{username}_user_profile_info.json`: Contains the raw JSON data for the user's profile.
    * The script will launch a Chromium browser window to perform the scraping.  You'll see the Twitter page load and scroll.

### 2. Analyzing Tweet Data (`twitter_scraping.ipynb`)

1.  **Open the Notebook:**
    *   Open `twitter_scraping.ipynb` in Jupyter Notebook or JupyterLab.

2.  **Set Input Filename (if necessary):**

    *  Locate the following lines in the notebook:
      ```python
      # --- Load Tweets Data ---
      json_filename = "JeffBezos_first_10_tweets.json"  # Store the filename
      with open(json_filename, "r") as json_file:
          data_tweets = json.load(json_file)
      ```
   *   Update `json_filename` to match the name of the JSON file containing the tweet data you want to analyze (the output from `twitter_scraping_cmds.py`).  You *usually* won't need to change this if you run the scraper and analyzer in the same directory, as the filenames are consistent.  But it's good practice to verify.

3.  **Run the Notebook:**
    *   Run all cells in the notebook (Cell -> Run All).

4.  **Output:**
    *   The notebook will:
        *   Load the tweet data from the specified JSON file.
        *   Extract relevant information (favorite count, text, etc.).
        *   Calculate average engagement metrics (favorites, retweets, replies).
        *   Display the analyzed data in a table.
        *   Highlight tweets with above-average engagement.
        *   Print the average engagement metrics.
        *   Create a CSV file named `{json_filename}_analyzed.csv` (e.g., `JeffBezos_first_10_tweets_analyzed.csv`) containing the analyzed data, including the average engagement metrics.

## Code Structure

### `twitter_scraping_cmds.py`

*   **`scrape_twitter_info(url, is_user_profile)`:** This function is the core of the scraper.  It takes the Twitter URL and a boolean flag (`is_user_profile`) as input.
    *   If `is_user_profile` is `True`, it scrapes the specified number of tweets from the user's timeline.
    *   If `is_user_profile` is `False`, it scrapes the user's profile information.
    *   It uses Playwright to interact with the Twitter page, handle scrolling, and intercept XHR (AJAX) requests to retrieve the data.
    *   It includes robust error handling, timeouts, and retries to deal with network issues and variations in Twitter's API responses.
*   **`get_username(url)`:** A helper function to extract the username from a Twitter URL.
*   **Configuration:** The `PROFILE_URL` and `NUM_POSTS_TO_RETRIEVE` variables control the scraping behavior.
*   **Main Execution:** The script calls `scrape_twitter_info` twice: once to get the tweets and once to get the user profile, saving the results to JSON files.

### `twitter_scraping.ipynb`

*   **`returnValueFromData(data, key)`:**  A helper function to navigate the nested JSON data structure and extract specific values based on a dot-separated key path (e.g., `"legacy.favorite_count"`).
*   **`analyze_tweets(df)`:**  This function takes a Pandas DataFrame of tweet data, calculates average engagement metrics, and adds boolean columns indicating whether each tweet's engagement is above average.
*   **Data Loading:** Loads tweet data from a JSON file.
*   **Data Extraction:**  Uses `returnValueFromData` and a dictionary (`tweet_key_to_key_mapping`) to extract the desired fields from the raw JSON data.
*   **Data Analysis:** Calls `analyze_tweets` to perform the engagement analysis.
*   **Data Display:**  Prints the analyzed data to the console, highlighting above-average tweets.
*   **Data Saving:** Saves the analyzed data (including calculated averages) to a CSV file.

## Notes and Considerations

*   **Twitter's Terms of Service:** Be aware of Twitter's terms of service and robots.txt file regarding scraping.  Automated scraping can be against their terms, and your account could be suspended.  Use this code responsibly and at your own risk.
*   **Rate Limiting:** Twitter's API has rate limits.  If you scrape too much data too quickly, you may be temporarily blocked.  The code includes some basic delays (`time.sleep`) to mitigate this, but you may need to adjust them.
*   **Headless Mode:** For production use or running the scraper in a server environment, consider running Playwright in headless mode (`headless=True` in `p.chromium.launch()`).
* **Dynamic Content and XHR:**  The scraper relies on intercepting XHR requests to get the data, as Twitter loads content dynamically.  Changes to Twitter's website structure or API could break the scraper.
*   **Authentication:** This scraper does *not* require you to log in to Twitter. However, this means you'll only be able to access publicly available data.  If you need to access private data or bypass certain rate limits, you'll need to implement authentication (which is significantly more complex).

## Disclaimer

This code is provided for educational and informational purposes only. The author is not responsible for any consequences resulting from the use of this code.  Use it responsibly and ethically, and respect Twitter's terms of service.